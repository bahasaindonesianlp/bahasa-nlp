{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After an uneventful first half, Romelu Lukaku gave United the lead on 55 minutes with a close-range volley.\n",
      "Sanchez was then fouled by Huddersfield defender Michael Hefele to win a penalty and the Chilean, a January signing from Arsenal, stepped up to take the spot-kick.\n",
      "The forward saw his low shot saved by Jonas Lossl, but made no mistake with the rebound to double United's lead on his home debut.\n"
     ]
    }
   ],
   "source": [
    "par_en = (\n",
    "    'After an uneventful first half, Romelu Lukaku gave United the lead on 55 minutes with a close-range volley.'\n",
    "    'Sanchez was then fouled by Huddersfield defender Michael Hefele to win a penalty and the Chilean, a January signing from Arsenal, stepped up to take the spot-kick.'\n",
    "    'The forward saw his low shot saved by Jonas Lossl, but made no mistake with the rebound to double United\\'s lead on his home debut.'\n",
    ")\n",
    "\n",
    "doc_en = nlp_en(par_en)\n",
    "\n",
    "for sentence in doc_en.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The forward saw his low shot saved by Jonas Lossl, but made no mistake with the rebound to double United's lead on his home debut."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'id'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-195591eeb13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ipy37/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ipy37/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'id'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "s = 'Galaxy Note 8, flagship terbaru dari Samsung, bisa ditebus dengan harga 11 juta rupiah (cashback 1 juta).'\n",
    "doc = nlp(s)\n",
    "print(type(doc))\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token-0 Galaxy\n",
      "token-1 Note\n",
      "token-2 8\n",
      "token-3 ,\n",
      "token-4 flagship\n",
      "token-5 terbaru\n",
      "token-6 dari\n",
      "token-7 Samsung\n",
      "token-8 ,\n",
      "token-9 bisa\n",
      "token-10 ditebus\n",
      "token-11 dengan\n",
      "token-12 harga\n",
      "token-13 11\n",
      "token-14 juta\n",
      "token-15 rupiah\n",
      "token-16 (\n",
      "token-17 cashback\n",
      "token-18 1\n",
      "token-19 juta\n",
      "token-20 )\n",
      "token-21 .\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(doc):\n",
    "    print(f'token-{i}', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seiring perkembangan, kebutuhan kini semakin mahal saja harganya. Lalu apa yang bisa kita lakukan? Alih-alih mengeluh sepanjang hari dan menyalahkan banyak orang, kini Anda harus memulai perubahan pada kehidupan Anda.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-63258277a40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdoc_par\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_par\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start."
     ]
    }
   ],
   "source": [
    "par = ('Seiring perkembangan, kebutuhan kini semakin mahal saja harganya. '\n",
    "'Lalu apa yang bisa kita lakukan? '\n",
    "'Alih-alih mengeluh sepanjang hari dan menyalahkan banyak orang, kini Anda harus memulai perubahan pada kehidupan Anda.'\n",
    ")\n",
    "print(par)\n",
    "\n",
    "doc_par = nlp(par)\n",
    "for sentence in doc_par.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sekalipun', 'bertutur', 'mungkin', 'khususnya', 'aku', 'bung', 'bermacam', 'terkira', 'antara', 'dipertanyakan']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.id.stop_words import STOP_WORDS\n",
    "\n",
    "# STOP WORDS is a set\n",
    "# convert to list\n",
    "print(list(STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saya True\n",
      "menyukai False\n",
      "kemewahan False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('saya menyukai kemewahan')\n",
    "for token in doc:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ancestors\n",
      "check_flag\n",
      "children\n",
      "cluster\n",
      "conjuncts\n",
      "dep\n",
      "dep_\n",
      "doc\n",
      "ent_id\n",
      "ent_id_\n",
      "ent_iob\n",
      "ent_iob_\n",
      "ent_kb_id\n",
      "ent_kb_id_\n",
      "ent_type\n",
      "ent_type_\n",
      "get_extension\n",
      "has_extension\n",
      "has_vector\n",
      "head\n",
      "i\n",
      "idx\n",
      "is_alpha\n",
      "is_ancestor\n",
      "is_ascii\n",
      "is_bracket\n",
      "is_currency\n",
      "is_digit\n",
      "is_left_punct\n",
      "is_lower\n",
      "is_oov\n",
      "is_punct\n",
      "is_quote\n",
      "is_right_punct\n",
      "is_sent_start\n",
      "is_space\n",
      "is_stop\n",
      "is_title\n",
      "is_upper\n",
      "lang\n",
      "lang_\n",
      "left_edge\n",
      "lefts\n",
      "lemma\n",
      "lemma_\n",
      "lex_id\n",
      "like_email\n",
      "like_num\n",
      "like_url\n",
      "lower\n",
      "lower_\n",
      "morph\n",
      "n_lefts\n",
      "n_rights\n",
      "nbor\n",
      "norm\n",
      "norm_\n",
      "orth\n",
      "orth_\n",
      "pos\n",
      "pos_\n",
      "prefix\n",
      "prefix_\n",
      "prob\n",
      "rank\n",
      "remove_extension\n",
      "right_edge\n",
      "rights\n",
      "sent\n",
      "sent_start\n",
      "sentiment\n",
      "set_extension\n",
      "shape\n",
      "shape_\n",
      "similarity\n",
      "string\n",
      "subtree\n",
      "suffix\n",
      "suffix_\n",
      "tag\n",
      "tag_\n",
      "tensor\n",
      "text\n",
      "text_with_ws\n",
      "vector\n",
      "vector_norm\n",
      "vocab\n",
      "whitespace_\n"
     ]
    }
   ],
   "source": [
    "token = doc[0]\n",
    "attributes = [attr for attr in dir(token) if not attr.startswith('_')]\n",
    "for attr in attributes:\n",
    "    print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          token   is_lower   is_title   is_upper   is_digit   is_punct\n",
      "             HP      False      False       True      False      False\n",
      "        Samsung      False       True      False      False      False\n",
      "         Galaxy      False       True      False      False      False\n",
      "           Note      False       True      False      False      False\n",
      "              8      False      False      False       True      False\n",
      "           bisa       True      False      False      False      False\n",
      "        ditebus       True      False      False      False      False\n",
      "         dengan       True      False      False      False      False\n",
      "          harga       True      False      False      False      False\n",
      "             11      False      False      False       True      False\n",
      "           juta       True      False      False      False      False\n",
      "         rupiah       True      False      False      False      False\n",
      "              (      False      False      False      False       True\n",
      "       cashback       True      False      False      False      False\n",
      "              1      False      False      False       True      False\n",
      "             jt       True      False      False      False      False\n",
      "              )      False      False      False      False       True\n",
      "              .      False      False      False      False       True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('HP Samsung Galaxy Note 8 bisa ditebus dengan harga 11 juta rupiah (cashback 1jt).')\n",
    "str_template = '{:>15} {:>10} {:>10} {:>10} {:>10} {:>10}'\n",
    "print(str_template.format('token', 'is_lower', 'is_title', 'is_upper', 'is_digit', 'is_punct'))\n",
    "for token in doc:\n",
    "    print(str_template.format(str(token),\n",
    "                              str(token.is_lower),\n",
    "                              str(token.is_title),\n",
    "                              str(token.is_upper),\n",
    "                              str(token.is_digit),\n",
    "                              str(token.is_punct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LOOKUP' from 'spacy.lang.id' (/Users/mtjokro/miniconda3/envs/ipy37/lib/python3.7/site-packages/spacy/lang/id/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-19d168e65476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOOKUP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LOOKUP' from 'spacy.lang.id' (/Users/mtjokro/miniconda3/envs/ipy37/lib/python3.7/site-packages/spacy/lang/id/__init__.py)"
     ]
    }
   ],
   "source": [
    "from spacy.lang.id import LOOKUP\n",
    "import random\n",
    "lemma_as_list = list(LOOKUP.items())\n",
    "samples = random.choices(lemma_as_list, k=20)\n",
    "for k, v in samples:\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.lang.id as id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spacy.lang.id' from '/Users/mtjokro/miniconda3/envs/ipy37/lib/python3.7/site-packages/spacy/lang/id/__init__.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tertidur tertidur\n",
      "tidur tidur\n",
      "tercyduk tercyduk\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('tertidur tidur tercyduk')\n",
    "for token in doc:\n",
    "    ori = token.text\n",
    "    lemma = token.lemma_  # token.lemma is integer index\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Indonesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = nlp(\"Saya berasal dari Australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tes:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya\n",
      "\n",
      "Saya\n",
      "berasal\n",
      "\n",
      "berasal\n",
      "dari\n",
      "\n",
      "dari\n",
      "Australia\n",
      "\n",
      "Australia\n"
     ]
    }
   ],
   "source": [
    "for token in tes:\n",
    "    print(token)\n",
    "    print(token.pos_)\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

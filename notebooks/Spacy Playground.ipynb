{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the existing Indonesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Indonesian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save it to a folder so we can manipulate it / add to it outside of the mainline Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('Indo')\n",
    "nlp = spacy.load('Indo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Saya berasal dari Australia\")\n",
    "\n",
    "tokens = [token.norm_ for token in doc]\n",
    "\n",
    "assert (tokens == ['saya','berasal','dari','australia'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 lemmas\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert (len(lemmas) == 4)\n",
    "    print (\"There are 4 lemmas\")\n",
    "except:\n",
    "    print(\"Lemmatization didn't work. There are only {} lemmas\".format(len(lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (lemmas[1] == 'asal'),\"Berasal should have been lemmatized to asal. Instead it was {}\".format(lemmas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Australia is not a proper noun, but it should be if the POS tagging worked",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b437822c989d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PROPN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'{} is not a proper noun, but it should be if the POS tagging worked'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Australia is not a proper noun, but it should be if the POS tagging worked"
     ]
    }
   ],
   "source": [
    "assert (pos[3] == 'PROPN'),'{} is not a proper noun, but it should be if the POS tagging worked'.format(doc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with the English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "English = en_core_web_sm.load()\n",
    "eng_doc = English(\"I am from Australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'ADP', 'PROPN']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in eng_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model on the Universal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pyconll.load_from_file('UD_Indonesian-GSD/id_gsd-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sembungan PROPN\n",
      "adalah AUX\n",
      "sebuah DET\n",
      "desa NOUN\n",
      "yang PRON\n",
      "terletak VERB\n",
      "di ADP\n",
      "kecamatan NOUN\n",
      "Kejajar PROPN\n",
      ", PUNCT\n",
      "kabupaten NOUN\n",
      "Wonosobo PROPN\n",
      ", PUNCT\n",
      "Jawa PROPN\n",
      "Tengah PROPN\n",
      ", PUNCT\n",
      "Indonesia PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sentence in train[0:1]:\n",
    "    for token in sentence:\n",
    "        print(token.form,token.upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Generated output file (4477 documents)\u001b[0m\n",
      "Spacy_Universal_Dependencies_Indonesian/id_gsd-ud-train.json\n",
      "\u001b[38;5;2m✔ Generated output file (557 documents)\u001b[0m\n",
      "Spacy_Universal_Dependencies_Indonesian/id_gsd-ud-test.json\n",
      "\u001b[38;5;2m✔ Generated output file (559 documents)\u001b[0m\n",
      "Spacy_Universal_Dependencies_Indonesian/id_gsd-ud-dev.json\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert 'UD_Indonesian-GSD/id_gsd-ud-train.conllu' 'Spacy_Universal_Dependencies_Indonesian' --converter conllu\n",
    "!python -m spacy convert 'UD_Indonesian-GSD/id_gsd-ud-test.conllu' 'Spacy_Universal_Dependencies_Indonesian' --converter conllu\n",
    "!python -m spacy convert 'UD_Indonesian-GSD/id_gsd-ud-dev.conllu' 'Spacy_Universal_Dependencies_Indonesian' --converter conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline: ['tagger', 'parser', 'ner']\n",
      "Starting with blank model 'id'\n",
      "Counting training words (limit=0)\n",
      "\n",
      "Itn    Dep Loss    NER Loss      UAS    NER P    NER R    NER F    Tag %  Token %  CPU WPS  GPU WPS\n",
      "---  ----------  ----------  -------  -------  -------  -------  -------  -------  -------  -------\n",
      "  0   83473.979       0.000   78.112    0.000    0.000    0.000   87.274  100.000     5887        0\n",
      "  1   66288.797       0.000   80.813    0.000    0.000    0.000   89.423  100.000     8411        0\n",
      "  2   62484.110       0.000   81.499    0.000    0.000    0.000   90.580  100.000     8537        0\n",
      "  3   60027.811       0.000   82.109    0.000    0.000    0.000   91.254  100.000     8149        0\n",
      "  4   57800.709       0.000   82.242    0.000    0.000    0.000   91.595  100.000     8171        0\n",
      "  5   55631.824       0.000   82.646    0.000    0.000    0.000   91.881  100.000     7554        0\n",
      "  6   55007.907       0.000   82.723    0.000    0.000    0.000   92.134  100.000     7384        0\n",
      "  7   53729.177       0.000   82.986    0.000    0.000    0.000   92.277  100.000     7374        0\n",
      "  8   52387.141       0.000   82.947    0.000    0.000    0.000   92.436  100.000     8320        0\n",
      "  9   50297.875       0.000   82.971    0.000    0.000    0.000   92.523  100.000     8355        0\n",
      "\u001b[38;5;2m✔ Saved model to output directory\u001b[0m\n",
      "models/model-final\n",
      "\u001b[2K\u001b[38;5;2m✔ Created best model\u001b[0m\n",
      "models/model-best\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train -n 10 id models 'Spacy_Universal_Dependencies_Indonesian/id_gsd-ud-train.json' 'Spacy_Universal_Dependencies_Indonesian/id_gsd-ud-dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('models/model-final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Saya berasal dari Australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya\n",
      "PRON\n",
      "berasal\n",
      "VERB\n",
      "Lemmatized to asal\n",
      "dari\n",
      "ADP\n",
      "Australia\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)\n",
    "    print(token.pos_)\n",
    "    if token.lemma_ != token.text:\n",
    "        print(\"Lemmatized to {}\".format(token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems to have worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Australia is not a proper noun, but it should be if the POS tagging worked. It is tagged as a NOUN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-e94f7daeca4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PROPN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'{} is not a proper noun, but it should be if the POS tagging worked. It is tagged as a {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Australia is not a proper noun, but it should be if the POS tagging worked. It is tagged as a NOUN"
     ]
    }
   ],
   "source": [
    "pos = [token.pos_ for token in doc]\n",
    "assert (pos[3] == 'PROPN'),'{} is not a proper noun, but it should be if the POS tagging worked. It is tagged as a {}'.format(doc[3],pos[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmm, still not tagging Australia as a proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
